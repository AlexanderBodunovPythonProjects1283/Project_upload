{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "п»ї#\n",
    "\n",
    "#РћРїСЂРµРґРµР»РµРЅРёРµ РєР»Р°СЃСЃР° Р·РІСѓРєР°\n",
    "\n",
    "#q0-q7\n",
    "\n",
    "#r = Recognize_sound()\n",
    "#r.single_file_input()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc, logfbank\n",
    "import librosa\n",
    "import cv2\n",
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "base_dir=\"C:/Users/nick/Desktop/my/PycharmProjects/treehouse_system/listen_sounds/\"\n",
    "#source_dir=base_dir+\"single_data/single.wav\"\n",
    "dest_dir=base_dir+\"single_data/single_harp/\"\n",
    "\n",
    "def write_signal(signal):\n",
    "    value = list(signal.values())[0]\n",
    "    values = [str(v) for v in value]\n",
    "    with open(dest_dir+\"signals/0.txt\", \"w\") as file:\n",
    "                file.write(\",\".join(values))\n",
    "\n",
    "def write_fft(fft):\n",
    "    data = list(fft.values())[0]\n",
    "    Y, freq = data[0], data[1]\n",
    "    Y=[str(v) for v in Y]\n",
    "    with open(dest_dir+\"fft/0.txt\", \"w\") as file:\n",
    "        file.write(\",\".join(Y))\n",
    "\n",
    "\n",
    "def write_fbank(fbank):\n",
    "    img_value=list(fbank.values())[0]\n",
    "    with open(dest_dir+\"fbank/0.txt\", \"w\") as file:\n",
    "                for row in img_value:\n",
    "                    row=[str(r) for r in row]\n",
    "                    file.write(\" \".join(row))\n",
    "                    file.write(\"\n",
    "\")\n",
    "\n",
    "\n",
    "def write_mfccs(mfccs):\n",
    "\n",
    "    img_value=list(mfccs.values())[0]\n",
    "    with open(dest_dir+\"mfccs/0.txt\", \"w\") as file:\n",
    "        for row in img_value:\n",
    "            row=[str(r) for r in row]\n",
    "            file.write(\" \".join(row))\n",
    "            file.write(\"\n",
    "\")\n",
    "\n",
    "\n",
    "def plot_signals(signals):\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1200, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle(\"Time Series\", size=16)\n",
    "    i = 0\n",
    "    print(len(list(signals.keys())))\n",
    "    for x in range(0,4):\n",
    "\n",
    "        for y in range(1200):\n",
    "            title=list(signals.keys())[i]\n",
    "            values=list(signals.values())[i]\n",
    "            axes[x,y].set_title(title)\n",
    "            axes[x,y].plot(values)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "\n",
    "            #print(\"Р¤РѕСЂРјР° СЃРёРіРЅР°Р»Р°: \",list(signals.values())[i].shape)\n",
    "\n",
    "            values=[str(v) for v in values]\n",
    "            with open(\"train/signals/{}.txt\".format(i), \"w\") as file:\n",
    "                file.write(\",\".join(values))\n",
    "            i += 1\n",
    "def plot_fft(fft):\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1200, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle(\"Fourier Transforms\", size=16)\n",
    "    i = 0\n",
    "\n",
    "    for x in range(0,4):\n",
    "        for y in range(1200):\n",
    "            data = list(fft.values())[i]\n",
    "            Y, freq = data[0], data[1]\n",
    "            print(Y,freq)\n",
    "            axes[x,y].set_title(list(fft.keys())[i])\n",
    "            axes[x,y].plot(freq, Y)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "\n",
    "            Y=[str(v) for v in Y]\n",
    "            with open(\"train/fft/{}.txt\".format(i), \"w\") as file:\n",
    "                file.write(\",\".join(Y))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "def plot_fbank(fbank):\n",
    "    min_value=np.inf\n",
    "    max_value=0\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1200, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle(\"Filter Bank Coefficients\", size=16)\n",
    "    i = 0\n",
    "    print(np.array((list(fbank.values())[0])).shape)\n",
    "    for x in range(0,4):\n",
    "        for y in range(1200):\n",
    "            img_value=list(fbank.values())[i]\n",
    "\n",
    "            axes[x,y].set_title(list(fbank.keys())[i])\n",
    "            axes[x,y].imshow(img_value,\n",
    "                    cmap=\"hot\", interpolation=\"nearest\")\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "            with open(\"train/fbank/{}.txt\".format(i), \"w\") as file:\n",
    "                for row in img_value:\n",
    "                    row=[str(r) for r in row]\n",
    "                    file.write(\" \".join(row))\n",
    "                    file.write(\"\n",
    "\")\n",
    "            i += 1\n",
    "    print(min_value,max_value)\n",
    "\n",
    "def plot_mfccs(mfccs):\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1200, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle(\"Mel Frequency Cepstrum Coefficients\", size=16)\n",
    "    i = 0\n",
    "    print(np.array((list(mfccs.values())[0])).shape)\n",
    "    for x in range(0,4):\n",
    "        for y in range(1200):\n",
    "            img_value=list(mfccs.values())[i]\n",
    "            axes[x,y].set_title(list(mfccs.keys())[i])\n",
    "            axes[x,y].imshow(img_value,\n",
    "                    cmap=\"hot\", interpolation=\"nearest\")\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            #cv2.imwrite(\"train/mfccs/{}.png\".format(i), img_value)\n",
    "            with open(\"train/mfccs/{}.txt\".format(i), \"w\") as file:\n",
    "                for row in img_value:\n",
    "                    row=[str(r) for r in row]\n",
    "                    file.write(\" \".join(row))\n",
    "                    file.write(\"\n",
    "\")\n",
    "            i += 1\n",
    "\n",
    "def calc_fft(y,rate):\n",
    "    n=len(y)\n",
    "    freq=np.fft.rfftfreq(n,d=1/rate)\n",
    "    Y=abs(np.fft.rfft(y)/n)\n",
    "    return(Y,freq)\n",
    "\n",
    "\n",
    "def envelope(y,rate,threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean=y.rolling(window=int(rate/10),min_periods=1,center=True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "#single_file_input()\n",
    "\n",
    "class Recognize_sound():\n",
    "    def __init__(self):\n",
    "\n",
    "        input_shape1 = (26, 99, 1)\n",
    "        input_shape2 = (13, 99, 1)\n",
    "        input_shape3 = (1, 66049)\n",
    "        input_shape4 = (1, 132096)\n",
    "\n",
    "        model1 = Sequential()\n",
    "        model1.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=input_shape1))\n",
    "        model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model1.add(BatchNormalization())\n",
    "        model1.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model1.add(BatchNormalization())\n",
    "        model1.add(Dropout(0.2))\n",
    "        model1.add(Flatten())\n",
    "        model1.add(Dense(128, activation=\"relu\"))\n",
    "        model1.add(Dense(8, activation=\"softmax\"))\n",
    "\n",
    "        model2 = Sequential()\n",
    "        model2.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=input_shape2))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model2.add(BatchNormalization())\n",
    "        model2.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model2.add(BatchNormalization())\n",
    "        model2.add(Dropout(0.2))\n",
    "        model2.add(Flatten())\n",
    "        model2.add(Dense(128, activation=\"relu\"))\n",
    "        model2.add(Dense(8, activation=\"softmax\"))\n",
    "\n",
    "        model3 = Sequential()\n",
    "        model3.add(LSTM(128, return_sequences=True, input_shape=input_shape3))\n",
    "        model3.add(LSTM(128, return_sequences=True))\n",
    "        model3.add(Dropout(0.5))\n",
    "        model3.add(TimeDistributed(Dense(64, activation=\"relu\")))\n",
    "        model3.add(TimeDistributed(Dense(32, activation=\"relu\")))\n",
    "        model3.add(TimeDistributed(Dense(16, activation=\"relu\")))\n",
    "        model3.add(TimeDistributed(Dense(8, activation=\"relu\")))\n",
    "        model3.add(Flatten())\n",
    "        model3.add(Dense(8, activation=\"softmax\"))\n",
    "\n",
    "        model4 = Sequential()\n",
    "        model4.add(LSTM(128, return_sequences=True, input_shape=input_shape4))\n",
    "        model4.add(LSTM(128, return_sequences=True))\n",
    "        model4.add(Dropout(0.5))\n",
    "        model4.add(TimeDistributed(Dense(64, activation=\"relu\")))\n",
    "        model4.add(TimeDistributed(Dense(32, activation=\"relu\")))\n",
    "        model4.add(TimeDistributed(Dense(16, activation=\"relu\")))\n",
    "        model4.add(TimeDistributed(Dense(8, activation=\"relu\")))\n",
    "        model4.add(Flatten())\n",
    "        model4.add(Dense(8, activation=\"softmax\"))\n",
    "\n",
    "        mergedOut = Add()([model1.output, model2.output, model3.output, model4.output])\n",
    "        output = Dense(8, activation=\"softmax\")(mergedOut)\n",
    "\n",
    "        newModel = Model([model1.input, model2.input, model3.input, model4.input], output)\n",
    "\n",
    "        newModel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        newModel.summary()\n",
    "\n",
    "        path = \"C:/Users/nick/Desktop/my/PycharmProjects/treehouse_system/listen_sounds/\"\n",
    "        weights = path + \"harpweights.h5\"\n",
    "        try:\n",
    "            newModel.load_weights(weights)\n",
    "            print(\"Р’РµСЃР° Р·Р°РіСЂСѓР¶РµРЅС‹\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.newModel=newModel\n",
    "        self.graph = tf.get_default_graph()\n",
    "\n",
    "    def harp_classifier(self):\n",
    "\n",
    "        def load_Y(start):\n",
    "            Y_train = np.array(Y[start:start + 200])\n",
    "            return Y_train\n",
    "\n",
    "        def extract_image_test(SOURCE_dir):\n",
    "            X_train = []\n",
    "            with open(SOURCE_dir, \"r\") as file:\n",
    "                data = file.read()\n",
    "            train_elem = []\n",
    "            rows = data.split(\"\n",
    "\")[:-1]\n",
    "            for r in rows:\n",
    "                row = [[float(el)] for el in r.split(\" \")]\n",
    "                train_elem.append(row)\n",
    "            X_train.append(train_elem)\n",
    "            X_train = np.array(X_train)\n",
    "\n",
    "            return X_train\n",
    "\n",
    "        def load_seq(SOURCE_dir):\n",
    "            X_train = []\n",
    "\n",
    "            with open(SOURCE_dir, \"r\") as file:\n",
    "                data = file.read()\n",
    "            data = data.split(\",\")\n",
    "            X_train.append([data])\n",
    "            X_train = np.array(X_train)\n",
    "            return X_train\n",
    "\n",
    "        base_dir = \"C:/Users/nick/Desktop/my/PycharmProjects/treehouse_system/listen_sounds/\"\n",
    "        source_dir = base_dir + \"single_data/single.wav\"\n",
    "        dest_dir = base_dir + \"single_data/single_harp/\"\n",
    "        num_samples = 10\n",
    "        num_rows = num_samples  # int(num_samples/6)\n",
    "        num_samples_per_file = 5  # 6\n",
    "\n",
    "        CHUNK = 1024\n",
    "        FORMAT = pyaudio.paInt16\n",
    "        CHANNELS = 2\n",
    "        RATE = 44100\n",
    "        RECORD_SECONDS = 3\n",
    "        # WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "        # DEST_DIR=\"single_data/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        folder_curr = 7\n",
    "        count_ = 0\n",
    "        files_counter = 800\n",
    "        while True:\n",
    "            count_ += 50\n",
    "            candidate = count_ // 2500\n",
    "            counter = 2500 - (count_ % 2500)\n",
    "            print(\"РћСЃС‚Р°Р»РѕСЃСЊ {}\".format(counter))\n",
    "            if counter == 2500:\n",
    "                frames = []\n",
    "                count_ = 0\n",
    "                files_counter += 1\n",
    "\n",
    "                p = pyaudio.PyAudio()\n",
    "                print(\"done 1\")\n",
    "                stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "                print(\"done 2\")\n",
    "                maxval = int(RATE / CHUNK * RECORD_SECONDS)\n",
    "\n",
    "                # bar = progressbar.ProgressBar(maxval=maxval,widgets=[progressbar.Bar(left=\"[\", marker=\"+\", right=\"]\")]).start()\n",
    "\n",
    "                old_d = 0\n",
    "                print(\"* recording\")\n",
    "                for i in range(0, maxval):\n",
    "                    data = stream.read(CHUNK)\n",
    "                    # bar.update(i)\n",
    "                    frames.append(data)\n",
    "                    d = int((i / maxval) * 30)\n",
    "                    if d > old_d:\n",
    "                        text = \"\n",
    "|{0}{1}|\".format(\"#\" * d, \"-\" * (30 - d))\n",
    "                        print(text)\n",
    "\n",
    "                print(\"* done recording\")\n",
    "                # bar.finish()\n",
    "\n",
    "                stream.stop_stream()\n",
    "                stream.close()\n",
    "                p.terminate()\n",
    "\n",
    "                wf = wave.open(source_dir, \"wb\")\n",
    "                wf.setnchannels(CHANNELS)\n",
    "                wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "                wf.setframerate(RATE)\n",
    "                wf.writeframes(b\"\".join(frames))\n",
    "                wf.close()\n",
    "\n",
    "                lables = []\n",
    "                file_names = []\n",
    "\n",
    "                signals = {}\n",
    "                fft = {}\n",
    "                fbank = {}\n",
    "                mfccs = {}\n",
    "\n",
    "                index_ = 0\n",
    "\n",
    "                # signal,rate=librosa.load(source_dir+wav_file,sr=44100, mono=True,offset=2+samp*7,duration=6)\n",
    "                signal, rate = librosa.load(source_dir, sr=44100, mono=True)\n",
    "\n",
    "                signals[index_] = signal\n",
    "                # print(\"Р”Р»РёРЅР° СЃРёРіРЅР°Р»Р° {}\".format(len(signal)))\n",
    "\n",
    "                fft[index_] = calc_fft(signal, rate)\n",
    "\n",
    "                bank = logfbank(signal[:rate], rate, nfilt=26, nfft=1103).T\n",
    "                fbank[index_] = bank\n",
    "\n",
    "                nel = mfcc(signal[:rate], rate, numcep=13, nfilt=26, nfft=1103).T\n",
    "                mfccs[index_] = nel\n",
    "\n",
    "                write_signal(signals)\n",
    "\n",
    "                write_fft(fft)\n",
    "                write_fbank(fbank)\n",
    "                write_mfccs(mfccs)\n",
    "\n",
    "                base_dir = \"C:/Users/nick/Desktop/my/PycharmProjects/treehouse_system/listen_sounds/\"\n",
    "                SOURCE_dir = base_dir + \"single_data/single_harp/\"\n",
    "                SOURCE_dir1 = SOURCE_dir + \"fbank/0.txt\"\n",
    "                SOURCE_dir2 = SOURCE_dir + \"mfccs/0.txt\"\n",
    "                SOURCE_dir3 = SOURCE_dir + \"fft/0.txt\"\n",
    "                SOURCE_dir4 = SOURCE_dir + \"signals/0.txt\"\n",
    "\n",
    "                t1 = time.time()\n",
    "                X_test_1 = extract_image_test(SOURCE_dir1)\n",
    "                print(time.time() - t1, \" СЃРµРєСѓРЅРґ\")\n",
    "                X_test_2 = extract_image_test(SOURCE_dir2)\n",
    "                print(time.time() - t1, \" СЃРµРєСѓРЅРґ\")\n",
    "                X_test_3 = load_seq(SOURCE_dir3)\n",
    "                print(time.time() - t1, \" СЃРµРєСѓРЅРґ\")\n",
    "                X_test_4 = load_seq(SOURCE_dir4)\n",
    "                print(time.time() - t1, \" СЃРµРєСѓРЅРґ\")\n",
    "                #print(X_test_1.shape, X_test_2.shape, X_test_3.shape, X_test_4.shape)\n",
    "                self.newModel._make_predict_function()\n",
    "\n",
    "                #Y = [to_categorical(ik) for ik in range(8)]\n",
    "                #for kl in Y:\n",
    "                    #score = self.newModel.evaluate([X_test_1, X_test_2, X_test_3, X_test_4], kl)\n",
    "                    #print (kl,score)\n",
    "\n",
    "                with self.graph.as_default():\n",
    "                    result = self.newModel.predict([X_test_1, X_test_2, X_test_3, X_test_4])\n",
    "                print(\"РљР»Р°СЃСЃ РјСѓР·С‹РєРё: \",max(result), np.argmax(result))\n",
    "                class_result = str(np.argmax(result))\n",
    "                dir=\"C:/Users/nick/Desktop/my/PycharmProjects/treehouse_system/listen_sounds/\"\n",
    "                with open(dir+\"result_sounds.txt\", \"w\")as file:\n",
    "                    file.write(\"s\" + class_result)\n",
    "\n",
    "                break\n",
    "                \n",
    "    def train():\n",
    "        SOURCE_dir=\"C:/Temp/train_harp_2/\"\n",
    "        SOURCE_dir1=SOURCE_dir+\"fbank/\"\n",
    "        SOURCE_dir2=SOURCE_dir+\"mfccs/\"\n",
    "        SOURCE_dir3=SOURCE_dir+\"fft/\"\n",
    "        SOURCE_dir4=SOURCE_dir+\"signals/\"\n",
    "\n",
    "        #dest_dir=\"C:/Temp/train_harp_pickle/\"\n",
    "\n",
    "        for train_step in range(100):\n",
    "            with open(\"statistics_1.txt\", \"a+\")as file:\n",
    "                file.write(\"С€Р°Рі {}\n",
    "\".format(train_step))\n",
    "            for batch in range(48):\n",
    "                t1=time.time()\n",
    "\n",
    "                print(\"РћР±СЂР°Р±Р°С‚С‹РІР°РµС‚СЃСЏ РІС‹Р±РѕСЂРєР° {} РёР· 240 \".format(batch))\n",
    "\n",
    "                X_train_1=extract_image_train(SOURCE_dir1,batch*50)\n",
    "                print(time.time()-t1,\" СЃРµРєСѓРЅРґ\")\n",
    "                X_train_2=extract_image_train(SOURCE_dir2,batch*50)\n",
    "                print(time.time()-t1,\" СЃРµРєСѓРЅРґ\")\n",
    "                X_train_3=load_seq(SOURCE_dir3,batch*50)\n",
    "                print(time.time()-t1,\" СЃРµРєСѓРЅРґ\")\n",
    "                X_train_4=load_seq(SOURCE_dir4,batch*50)\n",
    "                print(time.time()-t1,\" СЃРµРєСѓРЅРґ\")\n",
    "                Y_train=load_Y(batch*50)\n",
    "\n",
    "                X_train_1,X_train_2,X_train_3,X_train_4,Y_train=shuffle(X_train_1,X_train_2,X_train_3,X_train_4,Y_train)\n",
    "\n",
    "                #print(X_train_1.shape, X_train_2.shape, X_train_3.shape, X_train_4.shape, Y_train.shape)\n",
    "                if batch%2==1:\n",
    "                    newModel.fit([X_train_1, X_train_2,X_train_3,X_train_4], Y_train,batch_size=10,epochs=1)\n",
    "                else:\n",
    "                    score=newModel.evaluate([X_train_1, X_train_2,X_train_3,X_train_4], Y_train, batch_size=10)\n",
    "                    with open(\"statistics_1.txt\",\"a+\")as file:\n",
    "                        file.write(\"{}\n",
    "\".format(score))\n",
    "\n",
    "                    #newModel.save_weights(\"harpweights.h5\")\n",
    "                    print(\"РўРѕС‡РЅРѕСЃС‚СЊ РѕР±СѓС‡РµРЅРёСЏ РЅР° С‚РµСЃС‚РѕРІРѕР№ РІС‹Р±РѕСЂРєРµ: {}\".format(score))\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pass\n",
    "#with open(\"single_file_input.pkl\", \"wb\") as output:\n",
    "#r = Recognize_sound()\n",
    "#pickle.dump(r, output, pickle.HIGHEST_PROTOCOL)\n",
    "#r.single_file_input()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
